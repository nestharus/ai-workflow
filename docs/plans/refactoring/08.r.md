Plan 08 – docs/settings-patterns.md (Configuration & Settings
Patterns)
Intro Proposal:
NOTE TO GENERATOR (DO NOT EMIT IN OUTPUT):

You are generating `docs/settings-patterns.md`, one document in a FastAPI backend patterns series.

Global goals:
- Document how configuration and settings are modeled, validated, and used in the application.
- Align with Pydantic v2 / pydantic-settings best practices and `fastapi-best-practices.md` / `fastapi-best-practices-2.md`.
- Avoid line-number references; show concepts and patterns instead.

Specific scope for `settings-patterns.md`:
- Describe the structure of the Settings class (grouping related configuration: app, database, search, middleware, feature flags, etc.).
- Explain how environment variables (and optional `.env` files) map to settings fields.
- Document field validation patterns (e.g. `@field_validator` for URLs, pool sizes, timeouts) and custom settings-related exceptions.
- Describe secure handling of secrets (e.g. `repr=False`, schema hints, not logging secrets).
- Document environment-specific configuration behavior (dev/staging/prod/test) and the use of a debug flag.
- Explain how Settings are used at runtime (e.g. via a cached dependency, `app.state.settings`, pattern for overrides in tests).
- Mention feature flags and how they control behavior in other layers (e.g. error body inclusion, middleware toggles).

Out of scope:
- Factory wiring of Settings (covered in `factory-patterns.md`, though you can reference it).
- Detailed DB and connection pooling behavior (in `connection-pooling-patterns.md`).

Begin the documentation with the settings patterns heading, not this meta section.
Mistakes/Issues:
- Inline code references: The plan points out specific lines in core/settings.py (e.g. lines 93-94 for
  repr=False , lines 112-147 for validators). Instead of citing line numbers, summarize those patterns. For
  example: “Mark secret fields with repr=False and a password format in json_schema_extra so they
  don’t appear in logs or docs.” And “Use @field_validator for complex validations (e.g. ensuring
  surrealdb_url has a proper scheme or pool_size is positive).” This conveys the same info without
  tying it to line numbers.
- Credibility of “excellent” implementation: The doc says the current Settings implementation is
  excellent. That’s fine as praise, but if the code has any quirks, be careful. Ensure that what the doc calls out
  (like use of BaseSettings, environment loading, etc.) is truly done in code. If not, adjust the doc to reflect
  what should be done. For instance, if .env support isn’t actually configured, don’t say “all settings loaded
  from env files” unless that’s implemented via dotenv or similar. Make the documentation match the
  reality or the intended state after refactor.
- Custom exception classes for validation: It notes custom exceptions for settings validation (lines 43-73).
  Are these exceptions actually used to halt app startup on bad config? If so, document that pattern: e.g.
  “Raise a SettingsError if validation fails, to prevent the app from starting with an invalid configuration.”
  This helps readers understand how config errors are handled. If those exceptions aren’t fully integrated,
  consider whether to include that level of detail.
  Missing Best Practices:
- Using .env files: While it’s mentioned to use .env for local development, specify how that works with
  Pydantic. For example, if using pydantic_settings , you can supply env_file = ".env" in the
  Settings model or rely on environment variables directly. Clarifying that will help new contributors. (If the
  code uses pydantic.BaseSettings from v1, it auto-loads .env ; in v2’s pydantic-settings , you
  need to configure a source. Document whichever approach is in use.)
- Secret management: Beyond marking secrets repr=False , mention not to hard-code secrets and
  consider using a secrets manager for production (if relevant). Even a one-liner: “For production, sensitive
  credentials could be injected via environment or a secret manager – never checked into source.” This is
  somewhat obvious but aligns with best practices.
- Configuration override hierarchy: It might be worth stating how configuration is overridden in different
  environments (e.g. “settings can be overridden by env vars; test settings are provided by a fixture updating
  certain fields”). This is implied but not explicitly described. For completeness: “The Settings class reads from
  environment variables by default. We use different .env files or env var sets for dev/staging/production.
  In tests, we instantiate a Settings with overrides (see Testing Patterns doc).”
- Dynamic vs static settings: Some settings might change at runtime (not usually, they are generally static
  at startup). However, if any patterns like reloading config or using environment-specific logic exist, mention
  them. For instance, if debug=True should enable extra logging, note that pattern (and ensure the code
  does that). The plan touches on the debug flag usage for behavior differences – just make sure it’s clearly
  linked to actual effects (like “if debug=True , we might include error bodies in responses, etc.”).
- Settings and app startup: One best practice is to validate all critical settings at application startup (which
  Pydantic does on model creation) and fail fast. Your plan does highlight validation. Emphasize that: “The
  application will not start if required settings are invalid – this fail-fast approach prevents misconfiguration
  from causing runtime errors.” It reinforces why all this validation is done.