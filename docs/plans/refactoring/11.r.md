Plan 11 – docs/api-testing-patterns.md (API Testing Patterns)
Intro Proposal:
NOTE TO GENERATOR (DO NOT EMIT IN OUTPUT):

You are generating `docs/api-testing-patterns.md`, one document in a FastAPI backend patterns series.

Global goals:
- Document how to test the FastAPI application at different levels (unit, integration, E2E) using modern patterns.
- Encourage use of `app.dependency_overrides` and structured fixtures over ad-hoc monkeypatching.
- Align high-level testing ideas with `fastapi-best-practices.md` / `fastapi-best-practices-2.md`.

Specific scope for `api-testing-patterns.md`:
- Describe the overall testing architecture: unit tests, integration tests (in-process ASGI client), and E2E tests (live server / Docker).
- Document how fixtures create the app, provide test settings, and expose sync/async HTTP clients.
- Emphasize dependency overrides (`app.dependency_overrides`) as the primary mechanism for faking DB, services, and settings in tests.
- Describe strategies for mocking external dependencies (DB pools, external APIs, search, queues).
- Cover testing of validation errors, error responses, and OpenAPI-related behavior.
- Explain patterns for E2E tests: bringing up the full stack, waiting on readiness, capturing logs, and cleaning up.
- Provide guidance on test organization (folders, markers) and how to run fast vs full suites.

Out of scope:
- Implementation details of the app factory or connection pools (covered in other pattern docs except where directly relevant).
- Frontend testing; this document is focused on backend FastAPI tests.

The final documentation should begin with the API testing patterns heading, not this internal note.
Mistakes/Issues:
- Combining integration and E2E: The plan does a good job separating integration vs end-to-end (E2E)
  tests. One thing to check: does the codebase clearly separate “unit”, “integration”, and “e2e” directories and
  markers? The doc references tests/unit and marks E2E tests with @pytest.mark.e2e . Make sure the
  terminology in documentation matches the actual structure. For instance, if what the doc calls “integration”
  tests live in tests/unit (with an ASGI client), that might be a bit confusing naming. Ensure consistency
  (maybe they are considered unit tests that hit the app in memory). If needed, adjust wording. Possibly call
  them “integration tests” vs “full E2E tests” clearly.
- Line references in tests: The plan cites lines in test_health.py and conftest.py for things like
  waiting on Docker, or checking multiple assertions. Instead, summarize these patterns. For example: “Our
  E2E tests use Docker to run the whole stack. The test suite waits for the /health endpoint to report ready before
  running tests, and it cleans up containers afterward. (See our Docker compose config for details.)” This gives the
  idea without line-by-line. Similarly, mention that we use pytest-check for multiple assertions in one test
  (soft assertions), rather than pointing to line 20 of a file. The goal is that a new contributor reading this doc
  understands the testing philosophy without diving into test code.
- Deprecation of monkeypatch: It’s stated that the current approach uses monkeypatch in some places
  and that the plan is to migrate to dependency overrides. The documentation should strongly encourage the
  new approach. Make it clear that monkeypatch is considered an anti-pattern for FastAPI dependencies
  (except in rare cases), and that app.dependency_overrides is the preferred method. The anti-patterns
  section does list that, but emphasizing it in the main content (as it does in section 3) is good. Perhaps also
  note that using dependency overrides keeps tests closer to real usage and avoids tinkering with internals.
  Missing Best Practices:
- Parallel test execution: One thing not mentioned is test parallelization. If the test suite is ever run with
  multiple processes (e.g. using pytest -n auto via xdist), certain patterns (like using global app
  instances or a shared Docker DB) can break. The documentation might mention that currently tests assume
  sequential execution, or if using a fixed test database, tests are run serially (or isolated per class). If the
  project intends tests to run in parallel in the future, additional patterns (unique test DB per process, etc.)
  would be needed. Bringing this up depends on the project’s intentions for CI.
- API schema tests: A possible addition: testing the API schema itself. Sometimes a best practice is to
  validate that the OpenAPI documentation is correct or that all endpoints respond as documented. You could
  mention that as a pattern – e.g., using FastAPI’s app.openapi() in a test to ensure it generates without
  errors, or checking that all response_model fields appear. This might be extra, but it’s a nice-to-have test
  pattern for API consistency.
- Use of factories/data generation: The doc doesn’t mention how test data is created. If you have factories
  or sample data builders, mention them. For example, if using something like Factory Boy or just manual
  creation through the API, note how tests should create necessary data. e.g., “Integration tests call the API to
  set up any required data (avoiding direct DB insertion), to ensure the system is exercised end-to-end.” Or, if using
  direct DB seeding in tests, mention that pattern. Currently, the patterns don’t explicitly cover test data
  setup, which is a key part of testing.
- Consistency in fixtures naming: It references client and async_client fixtures. Ensure the
  documentation uses the same names as in code. If for instance the code uses test_client or similar,
  update the doc. This avoids confusion when someone tries to use those fixtures. It’s a minor consistency
  check but important for usability of the docs.
- CI integration: Possibly out of scope, but you might note that these testing patterns are automated in CI
  (Continuous Integration). E.g., “All tests (unit + integration + e2e) run in CI. Developers can run pytest -m
  "not e2e" for a quick local test, and include -m e2e when they need to run end-to-end tests (which require
  Docker).” This informs how to use these categories. It seems implied but stating it can help newcomers run
  the tests correctly.
  Each plan document above should be revised with these corrections and suggestions in mind. By
  addressing the mistakes (like over-reliance on code references) and filling in missing best practices, the
  FastAPI documentation will be both accurate and comprehensive for 2025 standards.